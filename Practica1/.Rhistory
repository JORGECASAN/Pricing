hist(DP_siniestros$Costes)
hist(DP_siniestros$N_sit)
DP2<- merge(DP_siniestros, Polizas2, by='ID_POL')
DP2<- DP2[!duplicated(DP2$ID_POL),] #Eliminamos los duplicados
DP2$Costes <- NULL #Nos interesa la columna antes nombrada como 'Coste', por lo que para no incurrir en multiolinealidad
modelo1 <- glm(Coste ~ Comb + Antigüedad + Forma_Pago + Edad_FMT + Valor_FMT + Potencia_FMT,
data = DP2, family = Gamma(link = 'log'))
summary(modelo1)# El intercept, la Antiguedad y Valor_FMT07.35-40k son los estadísticamente más significativos
#El AIC resultante es 16283 y el número de iteraciones Fisher de 10
coefficients(modelo1)
Polizas2 <-Polizas2 %>% mutate(Pred_costeprima = predict(modelo1, newdata = Polizas2, type = "response"))
set.seed(123)
names(DP2)
df_siniestro_freq <- merge(DP_siniestros, Polizas2)
df_siniestro_freq <- df_siniestro_freq[!duplicated(df_siniestro_freq$ID_POL), ] # Se eliminan los duplicados
df_siniestro_freq[is.na(df_siniestro_freq$N_sit),"N_sit"] <- 0 ## Se rellenan aquellos polizas sin accidentes a 0
summary(df_siniestro_freq)
modelo_freq <- glm(N_sit ~ Comb + Antigüedad + Forma_Pago + Edad_FMT + Valor_FMT + Potencia_FMT,
data = df_siniestro_freq, family = poisson(link = "log")) # Se escogen las mismas que para el coste
#Esta vez en family en vez de escoger la Gamma escogemos la Possion puesto que estamos calculando frecuencias
summary(modelo_freq) # Hay muchas variables que no son importantes
Polizas2 <- Polizas2 %>%
mutate(Pred_freq = predict(modelo_freq, newdata = Polizas2, type = "response"))
Polizas2 <- Polizas2 %>%
mutate(Prima_final = (Pred_freq * Pred_costeprima))
modelo_prima<- glm(Prima_final~Edad_FMT+Valor_FMT+Sexo+Comb+Potencia_FMT+Peso_FMT+Bonus_RC,data=Polizas2,
family = poisson(link='log'))
summary(modelo_prima) #La mayoría de los estadisticos son significativos
exp(modelo_prima$coefficients)
#Para calcular la tarifa final
names(modelo_prima)
View(modelo_prima)
View(modelo_prima)
#Nos instalamos todas las librerías necesarias para poder realizar la práctica
library(zoo)
library(caTools)
library(ROCR)
library(dplyr)
#Esta es la fecha actual
Fecha=Sys.Date()
Polizas=read.csv2('Policy_v1.csv')
Polizas2= read.csv2('Policy_v2.csv')
str(Polizas) #Vemos en qué formato están nuestras variables
str(Polizas2) #Lo mismo hacemos con el formato del siguiente archivo
summary(Polizas) #Nos proporciona información relevante sobre cómo está la distribución en los datos
summary(Polizas2) # Además ,el summary nos ofrece información relevante sobre variables numéricas
hist(Polizas$Potencia) #La mayoría de la potencia de los coches está entre 100 y 200 CV
hist(Polizas$Valor) # La mayoría de los coches tienen un valor de 16.000 € aproximadamente.
hist(Polizas$Peso)#La mayoría de los coches tiene un peso comprendido entre 1.000 y 2.000 kgs.
hist(Polizas2$Expuestos) #Vemos como el existen valores atípicos y como la frecuencia se distribuye hacia la derecha en la distribución
table(Polizas$Forma_Pago) #La mayoría de la forma de pago elegida por los asegurados es la forma A
table(Polizas$Antigüedad) #Existe gran volúmen de asegurados que tiene bastante tiempo el carnet de coche
table(Polizas$Sexo) # Más del doble de los asegurados son hombres
#Nos creamos una columna adicional que represente la fecha de originicación de la Póliza
Polizas$start_date2<- as.Date(Polizas$start_date)
Siniestros<- read.csv2('Claims_v1.csv')
str(Siniestros)
summary(Siniestros)
hist(Siniestros$Costes) #De nuevo tenemos que el coste de los siniestros es mucho inferior a los 10.000€
table(Siniestros$TipoSin)# La mayoría de la naturaleza de los siniestros son por RC Material y por Daños Propios
DP_siniestros <- siniestros[siniestros$TipoSin %in% c("Daños Propios"),] # Hacemos el filtro por los daños propios
DP_costes <- data.frame(aggregate(DP_siniestros$Costes, by = list(DP_siniestros$ID_POL), mean)) # Se hace la media de los daños propios por cada póliza
DP_numero <- data.frame(aggregate(sign(DP_siniestros$Costes), by = list(DP_siniestros$ID_POL), sum)) # Se suman las veces que la póliza ha sufrido un evento
names(DP_costes) <- c("ID_POL", "Coste") # Cambiamos los nombres
names(DP_numero) <- c("ID_POL", "N_sit") # Hacemos lo mismo para el número de siniestros
DP_siniestros <- siniestros[siniestros$TipoSin %in% c("Daños Propios"),] # Hacemos el filtro por los daños propios
siniestros<- read.csv2('Claims_v1.csv')
str(Siniestros)
summary(Siniestros)
hist(Siniestros$Costes) #De nuevo tenemos que el coste de los siniestros es mucho inferior a los 10.000€
table(Siniestros$TipoSin)# La mayoría de la naturaleza de los siniestros son por RC Material y por Daños Propios
DP_siniestros <- siniestros[siniestros$TipoSin %in% c("Daños Propios"),] # Hacemos el filtro por los daños propios
DP_costes <- data.frame(aggregate(DP_siniestros$Costes, by = list(DP_siniestros$ID_POL), mean)) # Se hace la media de los daños propios por cada póliza
DP_numero <- data.frame(aggregate(sign(DP_siniestros$Costes), by = list(DP_siniestros$ID_POL), sum)) # Se suman las veces que la póliza ha sufrido un evento
names(DP_costes) <- c("ID_POL", "Coste") # Cambiamos los nombres
names(DP_numero) <- c("ID_POL", "N_sit") # Hacemos lo mismo para el número de siniestros
DP_siniestros <- merge(DP_siniestros, DP_costes) # Se unen el df original con los costes medios por ID
DP_siniestros <- merge(DP_siniestros, DP_numero) # Se obtienen el df original, los costes medios por ID y la frecuencia de cada ID
summary(DP_siniestros) #Podemos observar la diferencia entre la media y la mediana, lo cual indica la presencia de outliers.
hist(DP_siniestros$Coste)
hist(DP_siniestros$N_sit)
DP2<- merge(DP_siniestros, Polizas2, by='ID_POL')
View(DP2)
View(DP2)
DP2<- DP2[!duplicated(DP2$ID_POL),] #Eliminamos los duplicados
DP2$Costes <- NULL #Nos interesa la columna antes nombrada como 'Coste', por lo que para no incurrir en multiolinealidad
View(DP2)
View(DP2)
modelo1 <- glm(Coste ~ Comb + Antigüedad + Forma_Pago + Edad_FMT + Valor_FMT + Potencia_FMT,
data = DP2, family = Gamma(link = 'log'))
summary(modelo1)# El intercept, la Antiguedad y Valor_FMT07.35-40k son los estadísticamente más significativos
#El AIC resultante es 16283 y el número de iteraciones Fisher de 10
coefficients(modelo1)
Polizas2 <-Polizas2 %>% mutate(Pred_costeprima = predict(modelo1, newdata = Polizas2, type = "response"))
set.seed(123)
names(DP2)
df_siniestro_freq <- merge(DP_siniestros, Polizas2)
df_siniestro_freq <- df_siniestro_freq[!duplicated(df_siniestro_freq$ID_POL), ] # Se eliminan los duplicados
df_siniestro_freq[is.na(df_siniestro_freq$N_sit),"N_sit"] <- 0 ## Se rellenan aquellos polizas sin accidentes a 0
summary(df_siniestro_freq)
modelo_freq <- glm(N_sit ~ Comb + Antigüedad + Forma_Pago + Edad_FMT + Valor_FMT + Potencia_FMT,
data = df_siniestro_freq, family = poisson(link = "log")) # Se escogen las mismas que para el coste
modelo_freq <- glm(N_sit ~ Comb + Antigüedad + Forma_Pago + Edad_FMT + Valor_FMT + Potencia_FMT,
data = df_siniestro_freq, family = poisson(link = "log")) # Se escogen las mismas que para el coste
#Esta vez en family en vez de escoger la Gamma escogemos la Possion puesto que estamos calculando frecuencias
summary(modelo_freq) # Hay muchas variables que no son importantes
#Esta vez en family en vez de escoger la Gamma escogemos la Possion puesto que estamos calculando frecuencias
summary(modelo_freq) # Hay muchas variables que no son importantes
Polizas2 <- Polizas2 %>%
mutate(Pred_freq = predict(modelo_freq, newdata = Polizas2, type = "response"))
Polizas2 <- Polizas2 %>%
mutate(Prima_final = (Pred_freq * Pred_costeprima))
modelo_prima<- glm(Prima_final~Edad_FMT+Valor_FMT+Sexo+Comb+Potencia_FMT+Peso_FMT+Bonus_RC,data=Polizas2,
family = poisson(link='log'))
summary(modelo_prima) #La mayoría de los estadisticos son significativos
Polizas2 %>% filter(Antigüedad==8,Forma_Pago=='A',Comb=='D',Sexo=='2', Bonus_RC=='40' ,Edad_FMT=='02.25-28',
Valor_FMT=='02.10-16k', Potencia_FMT=='02.70-90', Peso_FMT=='03. 1200',Carnet_FMT=='03.5-7')
Polizas2 %>% filter(Antigüedad==10,Forma_Pago=='A',Comb=='D',Sexo=='1', Bonus_RC=='50' ,Edad_FMT=='03.28-35',
Valor_FMT=='06.30-35k', Potencia_FMT=='06.150-200', Peso_FMT=='06. 1800',Carnet_FMT=='04.8-')
Polizas2 %>% filter(Antigüedad==0,Forma_Pago=='A',Comb=='D',Sexo=='1', Bonus_RC=='50' ,Edad_FMT=='05.50-65',
Valor_FMT=='01.<10k', Potencia_FMT=='02.70-90', Peso_FMT=='04. 1400',Carnet_FMT=='04.8-')
Polizas2 %>% filter(Antigüedad==10,Forma_Pago=='T',Comb=='D',Sexo=='2', Bonus_RC=='10' ,Edad_FMT=='01.18-25',
Valor_FMT=='02.10-16k', Potencia_FMT=='03.90-110', Peso_FMT=='03. 1200',Carnet_FMT=='01.0-1')
Polizas2 %>% filter(Antigüedad==10,Forma_Pago=='A',Comb=='D',Sexo=='1', Bonus_RC=='0', Edad_FMT=='03.28-35',
Valor_FMT=='04.20-24k', Potencia_FMT=='05.130-150', Peso_FMT=='05. 1600',Carnet_FMT=='02.2-4')
exp(modelo_prima$coefficients)
# Cargar librerias
library(CASdatasets) #libreria que contiene la BBDD danishuni
library(actuar) #libreria para análisis de riesgo operativo
library(fitdistrplus) #ajustar la distribucion con fitdis
library(ggplot2) # creacion de graficos
library(moments) # analisis asimetria y kurtosis
library(dplyr) #manipular datos
library(lubridate) # usar floor_date (agrupacion de fechas)
# Cargar los datos
data(danishuni)
# Tratamiento de los datos (agrupacion por mes)
# Objetivo obtener la frecuencia en las perdidas mensuales
df_perdidas <- danishuni %>%
group_by(floor_date(Date, 'months')) %>%
summarise(Freq = length(Loss), PerdidasM = sum(Loss))
# Cargar los datos
data(danishuni)
# Tratamiento de los datos (agrupacion por mes)
# Objetivo obtener la frecuencia en las perdidas mensuales
df_perdidas <- danishuni %>%
group_by(floor_date(Date, 'months')) %>%
summarise(Freq = length(Loss), PerdidasM = sum(Loss))
View(df_perdidas)
View(df_perdidas)
View(danishuni)
View(danishuni)
# A) Frecuencia de las perdidas mensuales
frecuencia <- df_perdidas$Freq
# Alta concentracion de valores en el centro de la distribucion
table(frecuencia)
# La media y la mediana se encuentran un poco lejanas
summary(frecuencia)
# Existe una alta varianza en la frecuencia mensual
var(frecuencia)
# Existe una asimetria positiva (sesgo a la derecha de la distribución)
skewness(frecuencia)
# Apuntamiento en el cuerpo de la distribucion
kurtosis(frecuencia)
# Graficar la distribucion frecuencia
png("./imagenes/distribucion frecuencia.png", width = 800, height = 500)
ggplot(df_perdidas) + geom_density(aes(Freq), fill = 'gray') +
xlab('Frecuencia') + ylab('Densidad')
dev.off()
ggplot(df_perdidas) + geom_density(aes(Freq), fill = 'gray') +
xlab('Frecuencia') + ylab('Densidad')
# Cuantiles
quantile(frecuencia, probs = c(0.05, 0.95))
quantile(frecuencia, seq(0,1, 0.20))
quantile(frecuencia, seq(0.9,1, 0.01))
# B) Severidad de las perdidas mensuales
severidad <- df_perdidas$PerdidasM
# Gran cantidad de valores en el centro de la distribución
table(severidad)
# La media y la mediana se encuentran lejanas
summary(severidad)
# Existe una alta varianza en la severidad mensual
var(severidad)
# Existe una asimetria positiva (sesgo a la derecha de la distribución)
skewness(severidad)
# Apuntamiento en el cuerpo de la distribucion
kurtosis(severidad)
# Graficar la distribucion severidad
png("./imagenes/distribucion severidad.png", width = 800, height = 500)
ggplot(df_perdidas) + geom_density(aes(PerdidasM), fill = 'gray') +
xlab('Severidad') + ylab('Densidad')
dev.off()
# Cuantiles
quantile(severidad, probs = c(0.05, 0.95))
quantile(severidad, seq(0,1, 0.20))
quantile(severidad, seq(0.9,1, 0.01))
# Distribucion Poisson, metodo máxima verosimilitud
fpois = fitdist(frecuencia, "pois", method = 'mle')
summary(fpois)
plot(fpois)
# Distribucion  Binomial Negativa, metodo máxima verosimilitud
fnbinom = fitdist(frecuencia, "nbinom", 'mle')
summary(fnbinom)
plot(fnbinom)
# La distribución se ajusta mejor a una binomial negativa
png("./imagenes/ajuste_frecuencia.png", width = 800, height = 500)
cdfcomp(list(fpois, fnbinom))
dev.off()
# Pruebas de bondad de ajuste
ba_discreta <- gofstat(list(fpois, fnbinom), chisqbreaks = c(7:16, 37), discrete = TRUE,
fitnames = c("Poisson", "Binomial Negativa"))
# Estadítico de bondad de ajuste: Chi test(pvalue)
ba_discreta$chisqpvalue #no se rechaza Ho en Bino. Nega son similares las distribuciones
# Criterio de bondad de ajuste (AIC y BIC)
ba_discreta$aic; ba_discreta$bic #se escoge la Bino. Neg por menor aic y bic
# Cargar librerias
library(CASdatasets) #libreria que contiene la BBDD danishuni
library(actuar) #libreria para análisis de riesgo operativo
library(fitdistrplus) #ajustar la distribucion con fitdis
library(ggplot2) # creacion de graficos
library(moments) # analisis asimetria y kurtosis
library(dplyr) #manipular datos
library(lubridate) # usar floor_date (agrupacion de fechas)
# Cargar los datos
data(danishuni)
# Tratamiento de los datos (agrupacion por mes)
# Objetivo obtener la frecuencia en las perdidas mensuales
df_perdidas <- danishuni %>%
group_by(floor_date(Date, 'months')) %>%
summarise(Freq = length(Loss), PerdidasM = sum(Loss))
# A) Frecuencia de las perdidas mensuales
frecuencia <- df_perdidas$Freq
# Alta concentracion de valores en el centro de la distribucion
table(frecuencia)
# La media y la mediana se encuentran un poco lejanas
summary(frecuencia)
# Existe una alta varianza en la frecuencia mensual
var(frecuencia)
# Existe una asimetria positiva (sesgo a la derecha de la distribución)
skewness(frecuencia)
# Apuntamiento en el cuerpo de la distribucion
kurtosis(frecuencia)
# Graficar la distribucion frecuencia
png("./imagenes/distribucion frecuencia.png", width = 800, height = 500)
# Graficar la distribucion frecuencia
png("./imagenes/distribucion frecuencia.png", width = 800, height = 500)
ggplot(df_perdidas) + geom_density(aes(Freq), fill = 'gray') +
xlab('Frecuencia') + ylab('Densidad')
dev.off()
# Cuantiles
quantile(frecuencia, probs = c(0.05, 0.95))
quantile(frecuencia, seq(0,1, 0.20))
quantile(frecuencia, seq(0.9,1, 0.01))
# B) Severidad de las perdidas mensuales
severidad <- df_perdidas$PerdidasM
# B) Severidad de las perdidas mensuales
severidad <- df_perdidas$PerdidasM
# Gran cantidad de valores en el centro de la distribución
table(severidad)
# La media y la mediana se encuentran lejanas
summary(severidad)
# Existe una alta varianza en la severidad mensual
var(severidad)
# Existe una asimetria positiva (sesgo a la derecha de la distribución)
skewness(severidad)
# Apuntamiento en el cuerpo de la distribucion
kurtosis(severidad)
# Graficar la distribucion severidad
png("./imagenes/distribucion severidad.png", width = 800, height = 500)
ggplot(df_perdidas) + geom_density(aes(PerdidasM), fill = 'gray') +
xlab('Severidad') + ylab('Densidad')
dev.off()
# Cuantiles
quantile(severidad, probs = c(0.05, 0.95))
quantile(severidad, seq(0,1, 0.20))
quantile(severidad, seq(0.9,1, 0.01))
# Distribucion Poisson, metodo máxima verosimilitud
fpois = fitdist(frecuencia, "pois", method = 'mle')
summary(fpois)
# Distribucion Poisson, metodo máxima verosimilitud
fpois = fitdist(frecuencia, "pois", method = 'mle')
summary(fpois)
plot(fpois)
# Distribucion  Binomial Negativa, metodo máxima verosimilitud
fnbinom = fitdist(frecuencia, "nbinom", 'mle')
summary(fnbinom)
plot(fnbinom)
# La distribución se ajusta mejor a una binomial negativa
png("./imagenes/ajuste_frecuencia.png", width = 800, height = 500)
# La distribución se ajusta mejor a una binomial negativa
png("./imagenes/ajuste_frecuencia.png", width = 800, height = 500)
cdfcomp(list(fpois, fnbinom))
dev.off()
# Pruebas de bondad de ajuste
ba_discreta <- gofstat(list(fpois, fnbinom), chisqbreaks = c(7:16, 37), discrete = TRUE,
fitnames = c("Poisson", "Binomial Negativa"))
# Estadítico de bondad de ajuste: Chi test(pvalue)
ba_discreta$chisqpvalue #no se rechaza Ho en Bino. Nega son similares las distribuciones
ba_discreta
# Estadítico de bondad de ajuste: Chi test(pvalue)
ba_discreta$chisqpvalue #no se rechaza Ho en Bino. Nega son similares las distribuciones
# Criterio de bondad de ajuste (AIC y BIC)
ba_discreta$aic; ba_discreta$bic #se escoge la Bino. Neg por menor aic y bic
# Resultado: La distribucion de frecuencia se ajusta mejor
# con una binomial negativa
size = fnbinom$estimate[1]
mu = fnbinom$estimate[2]
# GAMMA
fgam <- fitdist(severidad, "gamma", lower = 0)
summary(fgam)
plot(fgam)
# PARETO
fpar <- fitdist(severidad, "pareto", start = list(shape = 2, scale = 3))
summary(fpar)
plot(fpar)
# BURR
fburr <- fitdist(severidad, "burr", method = 'mle', start = list(shape1 = 2, shape2 = 2, scale = 2),
lower = c(0, 0, 0))
summary(fburr)
plot(fburr)
# Pruebas de bondad de ajuste
ba_continua <- gofstat(list(fgam, fpar, fburr), chisqbreaks = c(14:23), discrete = FALSE,
fitnames = c("GAMM", "Pareto", "Burr"))
# Estadítico de bondad de ajuste: Kolmogorov-Smirnov statistic
ba_continua$ks
ba_continua$kstest
# Criterio de bondad de ajuste
ba_continua$aic; ba_continua$bic
# Comprobacion grafica entre las distribuciones continuas
FDD = cdfcomp(list(fgam, fpar, fburr), xlogscale = TRUE,
ylab = "Probabilidad", datapch = ".",
datacol = "red", fitcol = c("black", "green", "blue"),
fitlty = 2:5, legendtext = c("Gamma", "Pareto", "Burr"),
main = "Ajuste pérdidas", plotstyle = "ggplot")
# La distribución se ajusta mejor a burr
png("./imagenes/ajuste_severidad.png", width = 800, height = 500)
FDD
dev.off()
# Resultado: La distribucion de severidad se ajusta mejor
# con una distribución burr
shape1 = fburr$estimate[1]
shape2 = fburr$estimate[2]
scale1 = fburr$estimate[3]
# Obtener la pérdida maxima por año
danishuni %>% group_by(floor_date(Date, 'year')) %>%
summarise(con = max(Loss))
# Exceso sobre umbral. Extrac. de Valores que exceden un umbral (POT)
u <- 54
exceden_umbral <- severidad[severidad > u]
exceden_umbral
#Visualizacion de las colas
n.u <- length(exceden_umbral) #n de casos que superan el umbral
surv.prob <- 1 - base::rank(exceden_umbral)/(n.u + 1)  #rank ofrece el n de orden
surv.prob
# Graficar la ditribución empricial y la de superviencia ajustada
plot(exceden_umbral, surv.prob, log = "xy", xlab = "Excesos",
ylab = "Probabilidades", ylim = c(0.01, 1))
#Determinamos los parámetros necesarios
alpha <- -cov(log(exceden_umbral), log(surv.prob)) / var(log(exceden_umbral))
alpha
x = seq(u, max(exceden_umbral), length = 100) #divide de u a max() 100 interv.
y = (x / u)^(-alpha)
lines(x, y)
#Función de distribución acumulada
prob <- rank(exceden_umbral) / (n.u + 1)
plot(exceden_umbral, prob, log = "x", xlab = "Excesos", ylab = "Probabilidades de no excesos")
y = 1 - (x / u)^(-alpha)
lines(x, y)
#Distribucion valores extremos generalizados (GEV)
nllik.gev <- function(par, data){
mu <- par[1]
sigma <- par[2]
xi <- par[3]
if ((sigma <= 0) | (xi <= -1))
return(1e6)
n <- length(data)
if (xi == 0)
n * log(sigma) + sum((data - mu) / sigma) +
sum(exp(-(data - mu) / sigma))
else {
if (any((1 + xi * (data - mu) / sigma) <= 0))
return(1e6)
n * log(sigma) + (1 + 1 / xi) *
sum(log(1 + xi * (data - mu) / sigma)) +
sum((1 + xi * (data - mu) / sigma)^(-1/xi))
}
}
# GEV
sigma.start <- sqrt(6) * sd(exceden_umbral) / pi
mu.start <- mean(exceden_umbral) + digamma(1) * sigma.start
fit.gev <- nlm(nllik.gev, c(mu.start, sigma.start, 0),
hessian = TRUE, data = exceden_umbral)
fit.gev
# Parametros: posición, escala y forma
fit.gev$estimate
sqrt(diag(solve(fit.gev$hessian)))
# B) Modelo Poisson-Generalizada de Pareto
nllik.gp <- function(par, u, data){
tau <- par[1]
xi <- par[2]
if ((tau <= 0) | (xi < -1))
return(1e6)
m <- length(data)
if (xi == 0)
m * log(tau) + sum(data - u) / tau
else {
if (any((1 + xi * (data - u) / tau) <= 0))
return(1e6)
m * log(tau) + (1 + 1 / xi) *
sum(log(1 + xi * (data - u) / tau))
}
}
# Obtención de los parámetros PARETO
tau.start <- mean(exceden_umbral) - u #valores medios entre el umbral y los excesos
fit.gp <- nlm(nllik.gp, c(tau.start, 0), u = u, hessian = TRUE,
data = exceden_umbral)
fit.gp
fit.gp$estimate
sqrt(diag(solve(fit.gp$hessian)))
#Intervalo de confianza del valor máximo del indice de cola al 95%
prof.nllik.gp <- function(par,xi, u, data)
nllik.gp(c(par,xi), u, data)
prof.fit.gp <- function(x)
-nlm(prof.nllik.gp, tau.start, xi = x, u = u, hessian = TRUE,
data = exceden_umbral)$minimum #ojo cambiar base de datos
vxi = seq(0,1.8,by = .025)
prof.lik <- Vectorize(prof.fit.gp)(vxi)
# Grafico intervalos de confianza
png("./imagenes/perfil_verosimilutd.png", width = 800, height = 500)
plot(vxi, prof.lik, type = "l", xlab = expression(xi),
ylab = "Profile log-likelihood")
opt <- optimize(f = prof.fit.gp, interval = c(0,3), maximum = TRUE)
opt
up <- opt$objective
abline(h = up, lty = 2)
abline(h = up - qchisq(p = 0.95, df = 1), col = "grey")
I <- which(prof.lik >= up - qchisq(p = 0.95, df = 1))
lines(vxi[I], rep(up - qchisq(p = 0.95, df = 1), length(I)),
lwd = 5, col = "grey")
abline(v = range(vxi[I]), col = "grey", lty = 2)
abline(v = opt$maximum, col = "grey")
dev.off()
# A) Q-Q Plot para la Dist. Generalizada de Pareto (DGP)
qqgpd <- function(data, u, tau, xi){
excess <- data[data > u]
m <- length(excess)
prob <- 1:m / (m + 1)
x.hat <- u + tau / xi * ((1 - prob)^-xi - 1)
ylim <- xlim <- range(x.hat, excess)
plot(sort(excess), x.hat, xlab = "Quantiles en la muestra",
ylab = "Quantiles ajustados", xlim = xlim, ylim = ylim)
abline(0, 1, col = "grey")
}
png("./imagenes/qq_plot.png", width = 800, height = 500)
qqgpd(severidad, u, fit.gp$estimate[1], fit.gp$estimate[2]) #umbral, tau y indice cola
dev.off()
#P-P Plot para la Dist. Generalizada de Pareto (DGP)
ppgpd <- function(data, u, tau, xi){
excess <- data[data > u]
m <- length(excess)
emp.prob <- 1:m / (m + 1)
prob.hat <- 1 - (1 + xi * (sort(excess) - u) / tau)^(-1/xi)
plot(emp.prob, prob.hat, xlab = "Probabilidades empiricas",
ylab = "Probabilidades ajustadas", xlim = c(0, 1),
ylim = c(0, 1))
abline(0, 1, col = "grey")
}
png("./imagenes/pp_plot.png", width = 800, height = 500)
ppgpd(severidad, u, fit.gp$estimate[1], fit.gp$estimate[2]) #umbral, tau y indice cola
dev.off()
perdidas_agregadas <- aggregateDist("simulation",
model.freq = expression(y = rnbinom(size = size, mu = mu)),
model.sev = expression(y = rburr(shape1 = shape1, shape2 = shape2, scale = scale1)),
nb.simul = 1000)
perdidas_agregadas
# Graficar la función de pérdidas acumuladas
png("./imagenes/funcion_perdidas_agregadas.png", width = 800, height = 500)
plot(perdidas_agregadas)
dev.off()
# Estadísticos de la nueva función:
summary(perdidas_agregadas)
# Deteminacion del Value at Risk al 0.99%
quantile(perdidas_agregadas, 0.99)
perdidas_agregadas <- aggregateDist("simulation",
model.freq = expression(y = rnbinom(size = size, mu = mu)),
model.sev = expression(y = rburr(shape1 = shape1, shape2 = shape2, scale = scale1)),
nb.simul = 1000)
perdidas_agregadas
# Graficar la función de pérdidas acumuladas
png("./imagenes/funcion_perdidas_agregadas.png", width = 800, height = 500)
plot(perdidas_agregadas)
dev.off()
# Estadísticos de la nueva función:
summary(perdidas_agregadas)
# Deteminacion del Value at Risk al 0.99%
quantile(perdidas_agregadas, 0.99)
perdidas_agregadas <- aggregateDist("simulation",
model.freq = expression(y = rnbinom(size = size, mu = mu)),
model.sev = expression(y = rburr(shape1 = shape1, shape2 = shape2, scale = scale1)),
nb.simul = 1000)
perdidas_agregadas
perdidas_agregadas <- aggregateDist("simulation",
model.freq = expression(y = rnbinom(size = size, mu = mu)),
model.sev = expression(y = rburr(shape1 = shape1, shape2 = shape2, scale = scale1)),
nb.simul = 1000)
perdidas_agregadas
# Deteminacion del Value at Risk al 0.99%
quantile(perdidas_agregadas, 0.99)
# Estadísticos de la nueva función:
summary(perdidas_agregadas)
# Deteminacion del Value at Risk al 0.99%
quantile(perdidas_agregadas, 0.99)
